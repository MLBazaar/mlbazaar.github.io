{
    "_id": "LL0_307_vowel",
    "about": {
        "problemID": "LL0_307_vowel_problem",
        "problemName": "LL0_307_vowel_problem",
        "problemDescription": "**Author**: Peter Turney (peter@ai.iit.nrc.ca)   \n**Source**: [UCI](https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/) - date unknown  \n**Please cite**:   \n\n# NAME: Vowel Recognition (Deterding data)\n\nSUMMARY: Speaker independent recognition of the eleven steady state vowels\nof British English using a specified training set of lpc derived log area\nratios.\n\nSOURCE: David Deterding  (data and non-connectionist analysis)\n        Mahesan Niranjan (first connectionist analysis)\n        Tony Robinson    (description, program, data, and results)\n\nTo contact Tony Robinson by electronic mail, use address\n\"tony@av-convex.ntt.jp\" until 1 June 1989, and \"ajr@dsl.eng.cam.ac.uk\"\nthereafter.\n\nMAINTAINER: Scott E. Fahlman, CMU\n\n## PROBLEM DESCRIPTION:\n\nThe problem is specified by the accompanying data file, \"vowel.data\".  This\nconsists of a three dimensional array: voweldata [speaker, vowel, input].\nThe speakers are indexed by integers 0-89.  (Actually, there are fifteen\nindividual speakers, each saying each vowel six times.)  The vowels are\nindexed by integers 0-10.  For each utterance, there are ten floating-point\ninput values, with array indices 0-9.\n\nThe problem is to train the network as well as possible using only on data\nfrom \"speakers\" 0-47, and then to test the network on speakers 48-89,\nreporting the number of correct classifications in the test set.\n\nFor a more detailed explanation of the problem, see the excerpt from Tony\nRobinson's Ph.D. thesis in the COMMENTS section.  In Robinson's opinion,\nconnectionist problems fall into two classes, the possible and the\nimpossible.  He is interested in the latter, by which he means problems\nthat have no exact solution.  Thus the problem here is not to see how fast\na network can be trained (although this is important), but to maximise a\nless than perfect performance.\n\n## METHODOLOGY:\n\nReport the number of test vowels classified correctly, (i.e. the number of\noccurences when distance of the correct output to the actual output was the\nsmallest of the set of distances from the actual output to all possible\ntarget outputs).\n\nThough this is not the focus of Robinson's study, it would also be useful\nto report how long the training took (measured in pattern presentations or\nwith a rough count of floating-point operations required) and what level of\nsuccess was achieved on the training and testing data after various amounts\nof training.  Of course, the network topology and algorithm used should be\nprecisely described as well.\n\n## VARIATIONS:\n\nThis benchmark is proposed to encourage the exploration of different node\ntypes.  Please theorise/experiment/hack.  The author (Robinson) will try to\ncorrespond by email if requested.  In particular there has been some\ndiscussion recently on the use of a cross-entropy distance measure, and it\nwould be interesting to see results for that.\n\n## RESULTS:\n\nHere is a summary of results obtained by Tony Robinson.  A more complete\nexplanation of this data is given in the exceprt from his thesis in the\nCOMMENTS section below.  The program used to obtain these results is in the\ncode directory, /afs/cs.cmu.edu/project as \"vowel.c\".\n\n+-------------------------+--------+---------+---------+\n|     | no. of | no.     | percent |\n| Classifier              | hidden | correct | correct |\n|     | units  |         |         | \n+-------------------------+--------+---------+---------+\n| Single-layer perceptron |  -     | 154     | 33      | \n| Multi-layer perceptron  | 88     | 234     | 51      |\n| Multi-layer perceptron  | 22     | 206     | 45      |\n| Multi-layer perceptron  | 11     | 203     | 44      | \n| Modified Kanerva Model  | 528    | 231     | 50      |\n| Modified Kanerva Model  | 88     | 197     | 43      | \n| Radial Basis Function   | 528    | 247     | 53      |\n| Radial Basis Function   | 88     | 220     | 48      | \n| Gaussian node network   | 528    | 252     | 55      |\n| Gaussian node network   | 88     | 247     | 53      |\n| Gaussian node network   | 22     | 250     | 54      |\n| Gaussian node network   | 11     | 211     | 47      | \n| Square node network     | 88     | 253     | 55      |\n| Square node network     | 22     | 236     | 51      |\n| Square node network     | 11     | 217     | 50      | \n| Nearest neighbour       |  -     | 260     | 56      | \n+-------------------------+--------+---------+---------+\n\n## Notes: \n\n1. Each of these numbers is based on a single trial with random starting\nweights.  More trials would of course be preferable, but the computational\nfacilities available to Robinson were limited.\n\n2. Graphs are given in Robinson's thesis showing test-set performance vs.\nepoch count for some of the training runs.  In most cases, performance\npeaks at around 250 correct, after which performance decays to different\ndegrees.  The numbers given above are final performance figures after about\n3000 trials, not the peak performance obtained during the run.\n\n## REFERENCES:\n\n[Deterding89] D. H. Deterding, 1989, University of Cambridge, \"Speaker\n Normalisation for Automatic Speech Recognition\", submitted for PhD.\n\n[NiranjanFallside88] M. Niranjan and F. Fallside, 1988, Cambridge University\n Engineering Department, \"Neural Networks and Radial Basis Functions in\n Classifying Static Speech Patterns\", CUED/F-INFENG/TR.22.\n\n[RenalsRohwer89-ijcnn] Steve Renals and Richard Rohwer, \"Phoneme\n Classification Experiments Using Radial Basis Functions\", Submitted to\n the International Joint Conference on Neural Networks, Washington,\n 1989.\n\n[RabinerSchafer78] L. R. Rabiner and R. W. Schafer, Englewood Cliffs, New\n Jersey, 1978, Prentice Hall, \"Digital Processing of Speech Signals\".\n\n[PragerFallside88] R. W. Prager and F. Fallside, 1988, Cambridge University\n Engineering Department, \"The Modified Kanerva Model for Automatic\n Speech Recognition\", CUED/F-INFENG/TR.6.\n\n[BroomheadLowe88] D. Broomhead and D. Lowe, 1988, Royal Signals and Radar\n Establishment, Malvern, \"Multi-variable Interpolation and Adaptive\n Networks\", RSRE memo, #4148.\n\n[RobinsonNiranjanFallside88-tr] A. J. Robinson and M. Niranjan and F. \n   Fallside, 1988, Cambridge University Engineering Department,\n \"Generalising the Nodes of the Error Propagation Network\",\n CUED/F-INFENG/TR.25.\n\n[Robinson89] A. J. Robinson, 1989, Cambridge University Engineering\n Department, \"Dynamic Error Propagation Networks\".\n\n[McCullochAinsworth88] N. McCulloch and W. A. Ainsworth, Proceedings of\n Speech'88, Edinburgh, 1988, \"Speaker Independent Vowel Recognition\n using a Multi-Layer Perceptron\".\n\n[RobinsonFallside88-neuro] A. J. Robinson and F. Fallside, 1988, Proceedings\n of nEuro'88, Paris, June, \"A Dynamic Connectionist Model for Phoneme\n Recognition.\n\n## COMMENTS:\n\n(By Tony Robinson)\n\nThe program supplied is slow.  I ran it on several MicroVaxII's for many\nnights.  I suspect that if I had spent more time on it, it would have been\npossible to get better results.  Indeed, my later code has a slightly\nbetter adaptive step size algotithm, but the old version is given here for\ncomatability with the stated performance values.  It is interesting that,\nfor this problem, the nearest neighbour clasification outperforms any of\nthe connectionist models.  This can be seen as a challange to improve the\nconnectionist performance.\n\nThe following problem description results and discussion is taken from my\nPhD thesis.  The aim was to demonstrate that many types of node can be\ntrained using gradient descent.  The full thesis will be available from me\nwhen it has been examined, say maybe July 1989.\n\n## Application to Vowel Recognition\n\nThis chapter describes the application of a variety of feed-forward networks\nto the task of recognition of vowel sounds from multiple speakers.  Single\nspeaker vowel recognition studies by Renals and Rohwer [RenalsRohwer89-ijcnn]\nshow that feed-forward networks compare favourably with vector-quantised\nhidden Markov models.  The vowel data used in this chapter was collected by\nDeterding [Deterding89], who recorded examples of the eleven steady state\nvowels of English spoken by fifteen speakers for a speaker normalisation\nstudy.  A range of node types are used, as described in the previous section,\nand some of the problems of the error propagation algorithm are discussed.\n\n## The Speech Data\n\n(An ascii approximation to) the International Phonetic Association (I.P.A.)\nsymbol and the word in which the eleven vowel sounds were recorded is given in\ntable 4.1.  The word was uttered once by each of the fifteen speakers.  Four\nmale and four female speakers were used to train the networks, and the other\nfour male and three female speakers were used for testing the performance.\n\n+-------+--------+-------+---------+\n| vowel |  word  | vowel |  word   | \n+-------+--------+-------+---------+\n|  i    |  heed  |  O    |  hod    |\n|  I    |  hid   |  C:   |  hoard  |\n|  E    |  head  |  U    |  hood   |\n|  A    |  had   |  u:   |  who'd  |\n|  a:   |  hard  |  3:   |  heard  |\n|  Y    |  hud   |       |         |\n+-------+--------+-------+---------+\n\nTable 4.1: Words used in Recording the Vowels\n\n## Front End Analysis\n\nThe speech signals were low pass filtered at 4.7kHz and then digitised to 12\nbits with a 10kHz sampling rate.  Twelfth order linear predictive analysis was\ncarried out on six 512 sample Hamming windowed segments from the steady part\nof the vowel.  The reflection coefficients were used to calculate 10 log area\nparameters, giving a 10 dimensional input space.  For a general introduction\nto speech processing and an explanation of this technique see Rabiner and\nSchafer [RabinerSchafer78].\n\nEach speaker thus yielded six frames of speech from eleven vowels.  This gave\n528 frames from the eight speakers used to train the networks and 462 frames\nfrom the seven speakers used to test the networks.\n\n## Details of the Models\n\nAll the models had common structure of one layer of hidden units and two\nlayers of weights.  Some of the models used fixed weights in the first layer\nto perform a dimensionality expansion [Robinson89:sect3.1], and the remainder\nmodified the first layer of weights using the error propagation algorithm for\ngeneral nodes described in [Robinson89:chap2].  In the second layer the hidden\nunits were mapped onto the outputs using the conventional weighted-sum type\nnodes with a linear activation function.  When Gaussian nodes were used the\nrange of influence of the nodes, w_ij1, was set to the standard deviation of\nthe training data for the appropriate input dimension.  If the locations of\nthese nodes, w_ij0, are placed randomly, then the model behaves like a\ncontinuous version of the modified Kanerva model [PragerFallside88].  If the\nlocations are placed at the points defined by the input examples then the\nmodel implements a radial basis function [BroomheadLowe88].  The first layer\nof weights remains constant in both of these models, but can be also trained\nusing the equations of [Robinson89:sect2.4].  Replacing the Gaussian nodes\nwith the conventional type gives a multilayer perceptron and replacing them\nwith conventional nodes with the activation function f(x) = x^2 gives a\nnetwork of square nodes.  Finally, dispensing with the first layer altogether\nyields a single layer perceptron.\n\nThe scaling factor between gradient of the energy and the change made to the\nweights (the `learning rate', `eta') was dynamically varied during training,\nas described in [Robinson89:sect2.5].  If the energy decreased this factor was\nincreased by 5%, if it increased the factor was halved.  The networks changed\nthe weights in the direction of steepest descent which is susceptible to\nfinding a local minimum.  A `momentum' term [RumelhartHintonWilliams86] is\noften used with error propagation networks to smooth the weight changes and\n`ride over' small local minima.  However, the optimal value of this term is\nlikely to be problem dependent, and in order to provide a uniform framework,\nthis additional term was not used.\n\n## Recognition Results\n\nThis experiment was originally carried out with only two frames of data from\neach word [RobinsonNiranjanFallside88-tr].  In the earlier experiment some\nproblems were encountered with a phenomena termed `overtraining' whereby the\nrecognition rate on the test data peaks part way through training then decays\nsignificantly.  The recognition rates for the six frames per word case are\ngiven in table 4.2 and are generally higher and show less variability than the\npreviously presented results.  However, the recognition rate on the test set\nstill displays large fluctuations during training, as shown by the plots in\n[Robinson89:fig3.2] Some fluctuations will arise from the fact that the\nminimum in weight space for the training set will not be coincident with the\nminima for the test set.  Thus, half the possible trajectories during learning\nwill approach the test set minimum and then move away from it again on the way\nto the training set minima [Mark Plumbley, personal communication].  In\naddition, continued training sharpens the class boundaries which makes the\nenergy insensitive to the class boundary position [Mahesan Niranjan, personal\ncommuniation].  For example, there are a large number planes defined with\nthreshold units which will separate two points in the input space, but only\none least squares solution for the case of linear units.\n\n+-------------------------+--------+---------+---------+\n|     | no. of | no.     | percent |\n| Classifier              | hidden | correct | correct |\n|     | units  |         |         | \n+-------------------------+--------+---------+---------+\n| Single-layer perceptron |  -     | 154     | 33      | \n| Multi-layer perceptron  | 88     | 234     | 51      |\n| Multi-layer perceptron  | 22     | 206     | 45      |\n| Multi-layer perceptron  | 11     | 203     | 44      | \n| Modified Kanerva Model  | 528    | 231     | 50      |\n| Modified Kanerva Model  | 88     | 197     | 43      | \n| Radial Basis Function   | 528    | 247     | 53      |\n| Radial Basis Function   | 88     | 220     | 48      | \n| Gaussian node network   | 528    | 252     | 55      |\n| Gaussian node network   | 88     | 247     | 53      |\n| Gaussian node network   | 22     | 250     | 54      |\n| Gaussian node network   | 11     | 211     | 47      | \n| Square node network     | 88     | 253     | 55      |\n| Square node network     | 22     | 236     | 51      |\n| Square node network     | 11     | 217     | 50      | \n| Nearest neighbour       |  -     | 260     | 56      | \n+-------------------------+--------+---------+---------+\n\nTable 4.2: Vowel classification with different non-linear classifiers\n\n## Discussion\n\nFrom these vowel classification results it can be seen that minimising the\nleast mean square error over a training set does not guarantee good\ngeneralisation to the test set.  The best results were achieved with nearest\nneighbour analysis which classifies an item as the class of the closest\nexample in the training set measured using the Euclidean distance.  It is\nexpected that the problem of overtraining could be overcome by using a larger\ntraining set taking data from more speakers.  The performance of the Gaussian\nand square node network was generally better than that of the multilayer\nperceptron.  In other speech recognition problems which attempt to classify\nsingle frames of speech, such as those described by McCulloch and Ainsworth\n[McCullochAinsworth88] and that of [Robinson89:chap7 and\nRobinsonFallside88-neuro], the nearest neighbour algorithm does not perform as\nwell as a multilayer perceptron.  It would be interesting to investigate this\ndifference and apply a network of Gaussian or square nodes to these problems.\n\nThe initial weights to the hidden units in the Gaussian network can be given a\nphysical interpretation in terms of matching to a template for a set of\nfeatures.  This gives an advantage both in shortening the training time and\nalso because the network starts at a point in weight space near a likely\nsolution, which avoids some possible local minima which represent poor\nsolutions.\n\nThe results of the experiments with Gaussian and square nodes are promising.\nHowever, it has not been the aim of this chapter to show that a particular\ntype of node is necessarily `better' for error propagation networks than the\nweighted sum node, but that the error propagation algorithm can be applied\nsuccessfully to many different types of node.\n\n\n# Notes:  \n* This is version 2. Version 1 is hidden because it includes a feature dividing the data in train and test set. In OpenML this information is explicitly available in the corresponding task.",
        "taskType": "classification",
        "taskSubType": "multiClass",
        "problemSchemaVersion": "3.0",
        "problemVersion": "1.0"
    },
    "inputs": {
        "data": [
            {
                "datasetID": "LL0_307_vowel_dataset",
                "targets": [
                    {
                        "targetIndex": 0,
                        "resID": "0",
                        "colIndex": 13,
                        "colName": "Class"
                    }
                ]
            }
        ],
        "dataSplits": {
            "method": "holdOut",
            "testSize": 0.2,
            "stratified": true,
            "numRepeats": 0,
            "randomSeed": 42,
            "splitsFile": "dataSplits.csv"
        },
        "performanceMetrics": [
            {
                "metric": "f1Macro"
            }
        ]
    },
    "expectedOutputs": {
        "predictionsFile": "predictions.csv"
    }
}